name: Cloud-Native DevOps Platform Deployment

on:
  workflow_dispatch:
  push:
    branches:
      - main

jobs:
  deploy:
    runs-on: ubuntu-latest
    env:
      # Map these once at job level so Terraform and Ansible read the same AWS context.
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Terraform Init
        run: terraform init
        working-directory: infra

      - name: Terraform Apply
        run: terraform apply -auto-approve -var="ssh_key_name=${{ secrets.SSH_KEY_NAME }}"
        working-directory: infra

      - name: Get EC2 public IP
        run: echo "EC2_IP=$(terraform output -raw public_ip)" >> $GITHUB_ENV
        working-directory: infra

      - name: Setup SSH
        run: |
          mkdir -p ~/.ssh
          printf "%s\n" "${{ secrets.EC2_SSH_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
             
          # Disable host key verification for CI since runners are  ephemeral
          cat <<EOF > ~/.ssh/config
          Host *
            StrictHostKeyChecking no
            UserKnownHostsFile=/dev/null
          EOF


      - name: Wait for SSH to be ready
        run: |
          # Retry SSH connection until the instance is fully ready.       
          echo "Waiting for SSH..."


          for i in {1..30}; do
            if ssh -o ConnectTimeout=5 -i ~/.ssh/id_rsa ubuntu@$EC2_IP "echo ready" 2>/dev/null; then
              echo "SSH is ready."
              exit 0
            fi

            echo "SSH not ready yet..."
            sleep 10     
          done

          echo "SSH not ready after timeout"
          exit 1


      - name: Configure server and deploy app
        run: ansible-playbook -i ansible/inventory.ini -e "ansible_host=$EC2_IP" ansible/docker.yml
